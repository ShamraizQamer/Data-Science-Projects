{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before dive-in to the NLP, Here are some information about the Irish Times\n",
    "\n",
    "### Irish Times\n",
    "### The Irish Times is an Irish daily broadsheet newspaper launched on 29 March 1859. The editor is Paul O'Neill who succeeded Kevin O'Sullivan on 5 April 2017; the deputy editor is Deirdre Veldon. The Irish Times is published every day except Sundays. It employs 420 people. More info in https://en.wikipedia.org/wiki/The_Irish_Times\n",
    "\n",
    "### Contents of the NLP\n",
    "### The following process will be undertaken in the NLP\n",
    "\n",
    "### Importing the libraries needed for the process\n",
    "### Downloading the StopWords and Lemmatizer\n",
    "### Perform Tokenization and Removal of StopWords\n",
    "### Initialization of Count Vectorizer\n",
    "### Split of Train and Test dataset\n",
    "### Creation of Model\n",
    "### Performing Predictions and analyze the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing the libraries needed for the process\n",
    "### Let's have a quick walkthrough about the libraries that gonna be used.\n",
    "\n",
    "### -> Pandas - Handling and Manipulation of Data Frame\n",
    "\n",
    "### -> NLTK (Natural Language Toolkit) - Powerful NLP libraries which contains packages to make machines understand human language\n",
    "\n",
    "### -> RE (Regular Expression) - Special sequence of characters that helps you match or find other strings or sets of strings.\n",
    "\n",
    "### > SKLearn (Scikit-learn) - Machine learning library for the Python programming language. It features various classification, regression and clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the Dataset\n",
    "### We use read_csv in Pandas to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_data = pd.read_csv(r'C:\\Users\\Shamraiz\\Desktop\\irishtimes-date-text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset consists of 3 columns and 1425460 rows.\n",
    "\n",
    "### The dataset consists of three columns:\n",
    "\n",
    "###       1. publish_date - The date when the news gets published. The format of the date is YYYYMMDD\n",
    "###       2. headline_category - The category of the news\n",
    "###       3. headline_text - The headline information of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is  (1425460, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_category</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19960102</td>\n",
       "      <td>news</td>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date headline_category  \\\n",
       "0      19960102              news   \n",
       "1      19960102              news   \n",
       "2      19960102              news   \n",
       "3      19960102              news   \n",
       "4      19960102              news   \n",
       "\n",
       "                                     headline_text  \n",
       "0         UUP sees possibility of voting Major out  \n",
       "1   Pubs targeted as curbs on smoking are extended  \n",
       "2  Papers reveal secret links with O'Neill cabinet  \n",
       "3      Domestic chaos as Italy takes EU presidency  \n",
       "4     Learning about the star to which we owe life  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The shape of the data is \",irish_data.shape)\n",
    "irish_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading the StopWords and Lemmatizer\n",
    "### Now, Let's download the stopwords and initialize the Lemmatizer. Note that, NLTK has a various language version of StopWords. You can have a look at the documentation of NLTK.\n",
    "\n",
    "### We will initialize the WordNetLemmatizer and also make a call to the function with a random words, because the lemmatizer will take some time to load the words into the workspace initially. So, it's my practice to run the function using initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ready'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we will use the below function to remove the stopwords and also special characters from the words using re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform Tokenization and Removal of StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(line):\n",
    "    word_tokens = word_tokenize(line)  \n",
    "    filtered_words = [re.sub('[^A-Za-z]+', '', w.lower()) for w in word_tokens if not w in stop_words]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will use the above function and create a new column called tokenized which will contain the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_data['tokenized'] = irish_data['headline_text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the function used to lemmatize the filtered words and also we will create a new column called 'lemmatized' which will contain the lemmatized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_words = []\n",
    "\n",
    "def lemmatize(line):\n",
    "    lem_words = []\n",
    "    \n",
    "    for word in line:\n",
    "        lem_word = lem.lemmatize(word,\"v\")\n",
    "        if len(lem_word) > 1:\n",
    "            lem_words.append(lem_word)\n",
    "            get_words.append(lem_word)\n",
    "    \n",
    "    return lem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_data['lemmatized'] = irish_data['tokenized'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = nltk.FreqDist(get_words)\n",
    "less_words = []\n",
    "\n",
    "for word in freq_words:\n",
    "    if freq_words[word]<=2:\n",
    "        less_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_words(lists):\n",
    "    filters = []\n",
    "    \n",
    "    for word in lists:\n",
    "        if word not in less_words:\n",
    "            filters.append(word)\n",
    "            \n",
    "    return ' '.join(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irish_data['filtered'] = irish_data['lemmatized'].apply(lambda x: remove_less_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now the data is filtered and lemmatized and stored in a column as you can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(irish_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialization of Count Vectorizer\n",
    "### Let's initialize the CountVectorizer. Below you can see that some parameters are initialized. We will go thorugh the parameters.\n",
    "\n",
    "### ngram_range = (1,2) => We are specifying the Count Vectorizer to use both unigrams and bigrams. Setting it to (2, 2) means only bigrams and (1, 1) means only unigrams.\n",
    "\n",
    "### lowercase=True => We are transforming the text into lowercase (Note: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )\n",
    "\n",
    "### stop_words='english' => We are specifying the stopwords as english to remove from the sentence. (Note: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )\n",
    "\n",
    "### tokenizer = token.tokenize => We are removing the special characters from the text using RegexpTokenizer. (Note: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "#CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "cv = CountVectorizer(ngram_range = (1,1))\n",
    "text_counts= cv.fit_transform(irish_data['filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split of Train and Test dataset\n",
    "### Now, the dataset is ready for training. So,Let's split the data into train and test data. We will use train_test_split to perform the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, irish_data['headline_category'], test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creation of Model\n",
    "### Now, Let's create the model. We will select LogisticRegression as our model for NLP. Below, is the model creation and training the model with the train input and train output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Performing Predictions and analyze the performance\n",
    "### Now, the model is trained with the dataset. Let's use the test dataset (X_test) to predict the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the prediction is performed and we will use the accuracy score function to calculate the accuracy level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on the dataset: {:.2f}\".format(accuracy_per*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on the dataset: 57.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
